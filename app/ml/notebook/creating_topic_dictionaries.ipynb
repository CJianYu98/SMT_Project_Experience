{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping CNA Articles by Topic\n",
    "There are 2 steps to scraping the CNA articles, in the first portion, we will be scraping article links only for each topic. After obtaining the links, we then pass through each link to sieve out the necessary information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install beautifulsoup4\n",
    "# ! pip install requests\n",
    "\n",
    "import urllib.request,sys,time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading csv file of a manually compiled list of links for different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>cna_search_link</th>\n",
       "      <th>num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>https://www.channelnewsasia.com/topic/sports?s...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>science</td>\n",
       "      <td>https://www.channelnewsasia.com/topic/science?...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>law</td>\n",
       "      <td>https://www.channelnewsasia.com/topic/law?sort...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic                                    cna_search_link  num_pages\n",
       "0   sports  https://www.channelnewsasia.com/topic/sports?s...         50\n",
       "1  science  https://www.channelnewsasia.com/topic/science?...          6\n",
       "2      law  https://www.channelnewsasia.com/topic/law?sort...         10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cna_links = pd.read_csv('../data/Trending Topics/links2.txt')\n",
    "cna_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Creating a function to scrape the URLs\n",
    "\n",
    "The main url we are scraping from changes for each topic, where articles tagged as under the topic will appear under the url. We will interate through the different pages of the url by changing the ending page number.\n",
    "\n",
    "The function loops through the pages using the pageNum parameter.\n",
    "\n",
    "Please update the headers variable with your own user agent. This prevents us from running into error 403: Forbidden when we scrape the URLs. \n",
    "\n",
    "\n",
    "To do so, \n",
    "1. Press F12 to navigate to the Chrome developer console.\n",
    "2. Type in <code>navigator.userAgent</code> in the console and execute it by hitting enter.\n",
    "3. Copy over your user agent and replace the value with your own in the headers dictionary.\n",
    "\n",
    "For more details, refer to https://stackoverflow.com/questions/38489386/python-requests-403-forbidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(topic, link, ending_page):\n",
    "    \n",
    "    #rename csv as name_urls.csv\n",
    "    with open(f'../data/Trending Topics/cna_urls/{topic}_cna_urls.csv', 'w', newline='') as file: #create a csv to input scrapped urls\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Page\", \"URL\"]) #create first header row with the column names as \"Page\" to indicate page number and \"URL\"\n",
    "    \n",
    "        for i in range(0, ending_page+1):\n",
    "            try:\n",
    "                url = link + str(i)\n",
    "                headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82 Safari/537.36\"}\n",
    "                \n",
    "                result = requests.get(url, headers=headers)\n",
    "\n",
    "                soup=BeautifulSoup(result.text,'html.parser')\n",
    "\n",
    "                article_urls = soup.findAll('a', attrs={'class' : 'h6__link h6__link-- list-object__heading-link'}) \n",
    "                for url in article_urls:\n",
    "                    #add row in csv with the page number and scrapped url\n",
    "                    writer.writerow([i, 'https://www.channelnewsasia.com'+url['href']])                                                                                        #link['href'] only gives us the relative path, not the absolute path, so we need to add the missing domain\n",
    "                                                                                    \n",
    "\n",
    "            except Exception as e:\n",
    "                print('exception')\n",
    "                error_type, error_obj, error_info = sys.exc_info()      # get the exception information\n",
    "                print ('ERROR FOR LINK:',url)                          #print the link that cause the problem\n",
    "                print (error_type, 'Line:', error_info.tb_lineno)     #print error info and line that threw the exception\n",
    "                continue                                              #ignore this page. Abandon this and go back.\n",
    "\n",
    "            time.sleep(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sports completed\n",
      "science completed\n",
      "law completed\n"
     ]
    }
   ],
   "source": [
    "for topic, link, num_pages in zip(cna_links[\"topic\"], cna_links[\"cna_search_link\"], cna_links[\"num_pages\"]):\n",
    "    get_urls(topic, link, num_pages)\n",
    "    print(f\"{topic} completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Getting information from each link\n",
    "\n",
    "Different functions are written to obtain specific information, by accessing the HTML tag on the CNA webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    title = soup.find(\"h1\", attrs={'h1 h1--page-title'})\n",
    "\n",
    "    return title\n",
    "        \n",
    "def get_text(soup):\n",
    "    text = ''\n",
    "    article = soup.find_all(\"div\", attrs={\"class\": \"text-long\"})\n",
    "\n",
    "    for i in range(len(article)):\n",
    "        each_class = article[i]\n",
    "        articleParagraph = each_class.find_all(\"p\")\n",
    "        for i in range(len(articleParagraph)):\n",
    "            text += articleParagraph[i].text + '\\n'\n",
    "            \n",
    "    return text\n",
    "\n",
    "def get_related_topics(soup):\n",
    "    other_keywords = []\n",
    "    \n",
    "    related_topics = soup.find(\"section\", attrs={\"class\": \"block block-layout-builder block-field-blocknodearticlefield-topics clearfix block--related-topics\"})\n",
    "    \n",
    "    try:\n",
    "        tags = related_topics.find_all(\"a\")\n",
    "\n",
    "        for tag in tags:\n",
    "            tag_text = tag.text\n",
    "            clean_tag = tag_text.replace(\"\\n \", \"\")\n",
    "            other_keywords.append(clean_tag)\n",
    "\n",
    "    except Exception as e: # when article does not have any related topic tags\n",
    "        other_keywords.append(\"\")\n",
    "\n",
    "    return other_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_data(url):\n",
    "    url_data = {}\n",
    "\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.82 Safari/537.36\"}\n",
    "    result = requests.get(url, headers=headers)\n",
    "    \n",
    "    if result.status_code != 404:\n",
    "    \n",
    "        soup=BeautifulSoup(result.text,'html.parser')\n",
    "\n",
    "        url_data['title'] = get_title(soup)\n",
    "        url_data['text'] = get_text(soup)\n",
    "        url_data['related_topics'] = get_related_topics(soup)\n",
    "\n",
    "    return url_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/Trending Topics/cna_urls\"\n",
    "\n",
    "# Obtaining article title, text and related topic keywords for each article \n",
    "for file in os.listdir(PATH):\n",
    "    topic = file.split(\"_\")[0] \n",
    "    url_df = pd.read_csv(os.path.join(PATH, file))\n",
    "    \n",
    "    url_df['data'] = url_df.apply(lambda x: get_url_data(x['URL']), axis=1)\n",
    "\n",
    "    url_df.to_csv(f\"../data/Trending Topics/cna_text/{topic}_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionary for each topic\n",
    "After collecting a repository of words from CNA articles as the source, we will select the top 1000 frequently used words for each topic as a dictionary of words for that topic. The dictionary will serve as a word bank for each topic to tag social media text to their topics.\n",
    "\n",
    "#### Processing text to remove unnecessary HTML tags and stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combining Stopwords from different libraries\n",
    "- NLTK\n",
    "- spaCy (cannot get the package installed)\n",
    "- Gensim\n",
    "- scikit-learn\n",
    "\n",
    "https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = {'‘ll', 'a', 'several', 'three', 'ten', 'few', 'you', 'since', 'until', 'under', 'namely', 'along', 'while', 'bottom', 'also', \"'ll\", 'this', '’re', 'anyone', 'whoever', 'already', 'their', 'who', 'third', 'therein', 'its', 'say', '’ve', 'fifteen', 'down', 'whole', 'doing', 'can', 'became', 'whereafter', 'eleven', 'during', 'call', 'formerly', 'via', 'on', 'almost', 'not', 'her', 'he', 'make', 'every', 'ca', 'as', 'nine', 'with', 'see', 'n‘t', 'anything', 'our', 'into', 'part', 'by', 'side', 'own', 'another', 'twenty', 'myself', 'other', 'next', 'over', 'regarding', 'afterwards', 'the', 'again', 'himself', 'nothing', 'even', 'an', 'that', \"'d\", 'meanwhile', '‘d', 'ever', 'has', 'show', '‘s', 'seems', 'so', 'back', 'everyone', '‘ve', 'nowhere', 'yet', 'she', 'both', 'my', 'per', 'be', 'thereupon', 'above', 'them', 'were', 'thereby', 'his', 'we', 'but', \"'s\", 'becomes', 'him', 'though', 'whether', 'each', 'fifty', 'yours', 'us', 'here', 'how', 'me', 'some', 'quite', 'anyway', 'former', 'wherever', 'onto', 'put', 'where', 'moreover', 'well', 'at', 'one', '’ll', 'around', 'amongst', 'no', 'is', 'mine', 'becoming', 'very', 'was', 'cannot', 'had', 'whenever', 'sometime', 'for', 'they', 'using', 'seemed', 'such', 'thru', 'among', 'hers', 'within', '‘re', 'various', 'front', '‘m', 'alone', 'do', 'then', 'about', 'behind', 'five', 'i', 'always', 'towards', 'least', 'themselves', 'whereby', 'name', 'go', 'whatever', 'upon', 'wherein', 'because', 'latterly', 'to', 'others', 'all', 'first', 'why', 'through', 'herself', 'what', 'somehow', 'in', 'might', 'been', 'mostly', 'elsewhere', 'unless', 'after', 'latter', 'throughout', 'if', 'four', 'hence', 'which', 'yourselves', 'often', 'am', 'perhaps', 'whom', 'are', 'seeming', 'top', 'something', 'amount', 'whereupon', 're', 'get', 'however', 'made', 'done', \"'re\", 'hereby', 'against', 'anyhow', 'become', 'does', 'keep', 'out', 'your', 'many', 'may', 'must', 'yourself', 'sixty', 'just', 'between', 'toward', 'whereas', \"'m\", 'last', 'twelve', 'empty', 'would', 'someone', 'otherwise', 'have', 'without', 'nobody', '’s', 'only', 'six', 'and', 'most', 'more', 'besides', 'two', 'although', 'seem', 'everywhere', 'eight', 'should', 'too', 'n’t', 'beforehand', 'now', 'together', 'somewhere', 'whose', 'enough', 'neither', 'nevertheless', '’m', 'still', 'same', 'sometimes', 'due', 'ours', 'either', 'being', 'hereupon', 'when', 'these', 'further', 'noone', 'any', 'therefore', 'there', \"n't\", 'beside', 'off', 'it', 'thereafter', 'never', 'those', 'from', 'really', 'could', 'whence', 'take', 'below', 'herein', 'beyond', '’d', 'move', 'thence', 'anywhere', 'thus', 'hereafter', 'everything', 'except', \"'ve\", 'whither', 'up', 'of', 'across', 'or', 'will', 'else', 'before', 'rather', 'none', 'full', 'indeed', 'hundred', 'serious', 'used', 'did', 'once', 'nor', 'forty', 'much', 'please', 'less', 'give', 'ourselves', 'itself', 'than'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_stopwords = STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stopwords from nltk: 179\n",
      "Number of stopwords from nltk + spaCy: 382\n",
      "Number of stopwords from nltk + spaCy + gensim: 412\n",
      "Total number of stopwords: 412\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stopwords from nltk:\", len(nltk_stopwords))\n",
    "\n",
    "nltk_stopwords.update(spacy_stopwords)\n",
    "print(\"Number of stopwords from nltk + spaCy:\", len(nltk_stopwords))\n",
    "\n",
    "nltk_stopwords.update(gensim_stopwords)\n",
    "print(\"Number of stopwords from nltk + spaCy + gensim:\", len(nltk_stopwords))\n",
    "\n",
    "combined_stopwords = set(nltk_stopwords)\n",
    "print(\"Total number of stopwords:\", len(combined_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated number of stopwords: 424\n"
     ]
    }
   ],
   "source": [
    "additional_stopwords = {'singapore', 'http', 'cna', 'said', 'download',  'app', 'subscribe', 'telegram', 'channel', 'latest', 'including', 'good'}\n",
    "combined_stopwords.update(additional_stopwords)\n",
    "\n",
    "print(\"Updated number of stopwords:\", len(combined_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    text = text.replace(\"{'title': <h1 class=\\\"h1 h1--page-title\\\">\", \" \").replace(\"\\n\", \" \").replace(\"</h1>\", \" \").replace(\"\\'text\\':\", \" \").replace(\"\\\\xa0\", \" \").replace(\"\\\\n\", \" \").replace(\"'related_topics': ['     \", \" \").replace(\"download app subscribe telegram channel latest\", \" \").replace(\"outbreak http asia\", \" \").replace(\"coronavirus outbreak http\", \" \").replace(\"http\", \" \")\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    lowercase_text = text.lower()\n",
    "    punctuations_removed = re.sub('[^a-z]', ' ', lowercase_text)\n",
    "    tokens = word_tokenize(punctuations_removed)\n",
    "    tokens = [token for token in tokens if len(token) !=1]\n",
    "    stopwords_removed = [token for token in tokens if token not in combined_stopwords]\n",
    "    lemmatized_tokens = [wnl.lemmatize(w) for w in stopwords_removed]\n",
    "\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the dictionary of words for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams(tokens):\n",
    "    bigram = ngrams(tokens, 2)\n",
    "    bigram_fdist = FreqDist(bigram).most_common(100)\n",
    "\n",
    "    bigram_list = []\n",
    "    for bigram in bigram_fdist:\n",
    "        bigram_list.append(bigram[0][0] + ' ' + bigram[0][1])\n",
    "\n",
    "    return bigram_list\n",
    "\n",
    "def extract_trigrams(tokens):\n",
    "    trigram = ngrams(tokens, 3)\n",
    "    trigram_fdist = FreqDist(trigram).most_common(100)\n",
    "    \n",
    "    trigram_list = []\n",
    "    for trigram in trigram_fdist:\n",
    "        trigram_list.append(trigram[0][0] + ' ' + trigram[0][1] + ' ' + trigram[0][2])\n",
    "\n",
    "    return trigram_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text from topic: law\n",
      "All words added\n",
      "\n",
      "Reading text from topic: science\n",
      "All words added\n",
      "\n",
      "Reading text from topic: sports\n",
      "All words added\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(\"../data/Trending Topics/cna_text\"): # read article texts for each topic\n",
    "    topic = file.split(\"_\")[0]\n",
    "    print(f\"Reading text from topic: {topic}\")\n",
    "\n",
    "    article_df = pd.read_csv(f\"../data/Trending Topics/cna_text/{file}\", index_col=0)\n",
    "    article_df[\"processed\"] = article_df[\"data\"].apply(processing)\n",
    "    \n",
    "    with open(f\"../data/Trending Topics/dictionary/{topic}.csv\", 'w') as file: # create new csv under dictionary folder to input top 1000 topical words for each topic\n",
    "        writer = csv.writer(file)\n",
    "        text = ''\n",
    "        for article_text, data in zip(article_df[\"processed\"], article_df[\"data\"]): # loop through all rows of article text\n",
    "            text += article_text # add all words from the article to a variable text \n",
    "\n",
    "        tokens = word_tokenize(text) # tokenize text and returns list\n",
    "        freq_dist = FreqDist(tokens).most_common(500) # returns a list of tuples (word, freq)\n",
    "        combined_list = [word_freq[0] for word_freq in freq_dist]\n",
    "        combined_list += extract_bigrams(tokens)\n",
    "        combined_list += extract_trigrams(tokens)\n",
    "\n",
    "        writer.writerow(combined_list)\n",
    "\n",
    "    print(\"All words added\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional topic words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Education\n",
    "\n",
    "ministry of education, gifted education programme, special assistance, financial assistance, poly, primary 1, primary 2, primary 3, primary 4, primary 5, primary 6, p1, p2, p3, p4, p5, p6, secondary 1, secondary 2, secondary 3, secondary 4, secondary 5, junior college, jc, uni, olevel, olevels, o level, o levels, nlevel, nevels , n level, n levels, a level, alevels, alevel, a levels, pe, nursery, primary education, homeschool, home based learning, mother tongue, primary school leaving examination, dsa, direct school admission, singapore polytechnic, singapore poly, temasek polytechnic, temasek poly, nanyang polytechnic, nanyang poly, sex education"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COVID-19\n",
    "covid, corona, coronavirus, corona virus, covid19, asymptomatic, case fatality rate, clinical trial, community spread, confirmed positive case, contact tracing, tracetogether, contactless, epidemic, pandemic, epidemiology, essential business, herd immunity, immunosuppressed, incubation, incubation period, lockdown, national emergency, outbreak, transmit, transmission, variant, alpha, beta, gamma, delta, omicron, respirator, frontline, frontline workers, frontliners, health workers, healthcare workers, self isolation, quarantine, social distancing, safe distancing, wuhan, symptomatic, vaccine, vaccination, pfizer, moderna, sinovac, booster, work from home, working from home, border, world health organisation, vtl, vaccinated travel lane, travel bubble, travel restrictions, border closure, border reopening, ease restrictions, tighten restrictions, ministry of health, moh, local cases, imported cases, pcr test, art test, test kit, testing centre, swab test, positive cases, surgical masks, medical masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overlapping words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/Trending Topics/dictionary/art.csv') as f:\n",
    "    art_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/covid19.csv') as f:\n",
    "    covid19_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/education.csv') as f:\n",
    "    edu_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/environment.csv') as f:\n",
    "    env_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/fashion.csv') as f:\n",
    "    fashion_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/food.csv') as f:\n",
    "    food_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/health.csv') as f:\n",
    "    health_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/politics.csv') as f:\n",
    "    politics_dict = set([i for i in f][0].split(\",\"))\n",
    "with open('../data/Trending Topics/dictionary/technology.csv') as f:\n",
    "    tech_dict = set([i for i in f][0].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_dicts = [('art', art_dict), ('covid19', covid19_dict), ('education', edu_dict), ('environment', env_dict), ('fashion', fashion_dict), ('food', food_dict), ('health', health_dict), ('politics', politics_dict), ('technology', tech_dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlaps(topic_dict):\n",
    "    topic_overlapped_list = []\n",
    "    for i in range(len(topic_dict)):\n",
    "        topic_overlapped = set()\n",
    "        for k in range(i+1, len(topic_dict)):\n",
    "            overlapping_words = list(topic_dict[i][1].intersection(topic_dict[k][1]))\n",
    "            topic_overlapped.update(overlapping_words)\n",
    "            # print(f\"{topic_dict[i][0]} & {topic_dict[k][0]}: {overlapping_words}\")\n",
    "            # print(\"\")\n",
    "            topic_overlapped_list.append(topic_overlapped)\n",
    "    return topic_overlapped_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperation",
     "evalue": "not writable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_61420/2383398340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mart_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mart_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mart_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moverlapped_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mart_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: not writable"
     ]
    }
   ],
   "source": [
    "overlapped_words = find_overlaps(topic_dicts)\n",
    "overlapped_words[1]\n",
    "\n",
    "with open('../data/Trending Topics/dictionary/art.csv') as f:\n",
    "    writer = csv.writer(f)\n",
    "    art_dict = set([i for i in f][0].split(\",\"))\n",
    "    art_dict = [word for word in art_dict if word not in overlapped_words[1]]\n",
    "    writer.writerow(art_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
